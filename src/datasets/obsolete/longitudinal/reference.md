# Longitudinal Reference

<!-- toc -->

# Introduction

{{#include ./intro.md}}

# Data Reference

## Sampling

### Pings Within Last 6 Months

The `longitudinal` filters to `main` pings from within the last 6 months.

### 1% Sample

The longitudinal dataset samples down to 1% of all clients in the above sample.
The sample is generated by the following process:

- hash the `client_id` for each ping from the last 6 months.
- project that hash onto an integer from 1:100, inclusive
- filter to pings with `client_id`s matching a 'magic number' (in this case 42)

This process has a couple of nice properties:

- The sample is consistent over time.
  The `longitudinal` dataset is regenerated weekly.
  The clients included in each run are very similar with this process.
  The only change will come from never-before-seen clients,
  or clients without a ping in the last 180 days.
- We don't need to adjust the sample as new clients enter or exit our pool.

More practically,
the sample is created by filtering to pings with `main_summary.sample_id == 42`.
If you're working with `main_summary`,
you can recreate this sample by doing this filter manually.

## Scheduling

The `longitudinal` job is run weekly, early on Sunday morning UTC.
The job is scheduled on [Airflow](https://github.com/mozilla/telemetry-airflow).
The DAG is [here](https://github.com/mozilla/telemetry-airflow/blob/54cffc42a2ca24e46056b7030735f0d4d093c0c7/dags/longitudinal.py).

## Schema

`TODO(harter)`: https://bugzilla.mozilla.org/show_bug.cgi?id=1361862

# Code Reference

This dataset is generated by
[telemetry-batch-view](https://github.com/mozilla/telemetry-batch-view/blob/master/GRAVEYARD.md#longitudinal).
Refer to this repository for information on how to run or augment the dataset.
